{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('8taijiquan.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values in the 'class' column\n",
    "unique_classes = df['class'].unique()\n",
    "\n",
    "# Display the unique values\n",
    "print(\"Unique Classes:\", unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of classes\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['class']=='Horse Stance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('pastel')\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Calculate the class frequencies and sort by frequency\n",
    "class_counts = df['class'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Reorder the DataFrame based on class_counts\n",
    "df_sorted = df[df['class'].isin(class_counts.index)]\n",
    "\n",
    "# Plot the countplot with the sorted DataFrame\n",
    "sns.countplot(data=df_sorted, x='class', palette='pastel', order=class_counts.index)\n",
    "\n",
    "plt.xlabel('Stance', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Distribution of Taijiquan', fontsize=16)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "for p in plt.gca().patches:\n",
    "    plt.gca().text(p.get_x() + p.get_width() / 2., p.get_height(), f'{int(p.get_height())}',\n",
    "                   ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df.drop('class', axis=1),  # Features (X)\n",
    "    df['class'],                # Target variable (y)\n",
    "    test_size=0.3,              # Percentage of data for the validation set\n",
    "    random_state=42,            # Random state\n",
    "    stratify=df['class']         # Class distribution in the splits\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,                     # Features (X) after the first split\n",
    "    y_temp,                     # Target variable (y) after the first split\n",
    "    test_size=0.5,              # Percentage of data for the test set (relative to X_temp)\n",
    "    random_state=42,            # Random state\n",
    "    stratify=y_temp              # Class distribution in the splits\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landmark-based Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply landmark-based augmentation\n",
    "def apply_landmark_augmentation(landmarks, angle_range=(-10, 10), scale_range=(0.9, 1.1)):\n",
    "    augmented_landmarks = landmarks.copy()\n",
    "    num_landmarks = landmarks.shape[0]\n",
    "\n",
    "    # Apply augmentation to each landmark\n",
    "    for i in range(num_landmarks):\n",
    "        angle = np.random.uniform(angle_range[0], angle_range[1])\n",
    "        rotation_matrix = np.array([[np.cos(np.radians(angle)), -np.sin(np.radians(angle))],\n",
    "                                    [np.sin(np.radians(angle)), np.cos(np.radians(angle))]])\n",
    "        augmented_landmarks[i, :2] = np.dot(augmented_landmarks[i, :2], rotation_matrix.T)\n",
    "        scale_factor = np.random.uniform(scale_range[0], scale_range[1])\n",
    "        augmented_landmarks[i, :2] *= scale_factor\n",
    "\n",
    "    return augmented_landmarks.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply landmark-based augmentation to X_train\n",
    "X_train_augmented = []\n",
    "for index, row in X_train.iterrows():\n",
    "    landmarks = np.array(row).reshape(-1, 4)  \n",
    "    augmented_landmarks = apply_landmark_augmentation(landmarks)\n",
    "    X_train_augmented.append(augmented_landmarks)\n",
    "\n",
    "X_train_augmented = pd.DataFrame(X_train_augmented, columns=X_train.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode class labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FNN model with modifications\n",
    "def create_fnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001), input_shape=input_shape))\n",
    "    model.add(Dropout(0.5))  # Adjust dropout rate\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dropout(0.5))  # Adjust dropout rate\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the FNN model with early stopping\n",
    "input_shape_fnn = (X_train_augmented.shape[1],)\n",
    "fnn_model = create_fnn_model(input_shape_fnn)\n",
    "\n",
    "# Number of training sessions\n",
    "num_training_sessions = 2  # Adjust as needed\n",
    "\n",
    "# List to store historical performance\n",
    "historical_performance = []\n",
    "\n",
    "for session in range(num_training_sessions):\n",
    "    print(f\"Training Session: {session + 1}/{num_training_sessions}\")\n",
    "\n",
    "    # Train the FNN model and record history\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = fnn_model.fit(X_train_augmented, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_val, y_val_encoded), callbacks=[early_stopping])\n",
    "\n",
    "    # Append the history to the list\n",
    "    historical_performance.append(history.history)\n",
    "\n",
    "# Access historical performance\n",
    "for session, history in enumerate(historical_performance):\n",
    "    print(f\"\\nTraining Session: {session + 1}/{num_training_sessions}\")\n",
    "    print(f\"Final Training Loss: {history['loss'][-1]}\")\n",
    "    print(f\"Final Validation Loss: {history['val_loss'][-1]}\")\n",
    "    print(f\"Final Training Accuracy: {history['accuracy'][-1]}\")\n",
    "    print(f\"Final Validation Accuracy: {history['val_accuracy'][-1]}\")\n",
    "\n",
    "    # Plot loss over epochs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f\"Training Session: {session + 1}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy over epochs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f\"Training Session: {session + 1}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the FNN model on the test set\n",
    "test_loss_fnn, test_accuracy_fnn = fnn_model.evaluate(X_test, y_test_encoded)\n",
    "print(f'Test Accuracy (FNN): {test_accuracy_fnn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for FNN\n",
    "y_pred_fnn = fnn_model.predict(X_test)\n",
    "y_pred_classes_fnn = np.argmax(y_pred_fnn, axis=1)\n",
    "conf_matrix_fnn = confusion_matrix(y_test_encoded, y_pred_classes_fnn)\n",
    "print(conf_matrix_fnn)\n",
    "sns.heatmap(conf_matrix_fnn, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (FNN)')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report for FNN\n",
    "class_report_fnn = classification_report(y_test_encoded, y_pred_classes_fnn)\n",
    "print('Classification Report (FNN):')\n",
    "print(class_report_fnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "X_train_augmented = np.array(X_train_augmented)\n",
    "y_train_encoded = np.array(y_train_encoded)\n",
    "\n",
    "# Define the number of folds\n",
    "n_splits = 5  \n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results for each fold\n",
    "all_train_accuracy = []\n",
    "all_val_accuracy = []\n",
    "\n",
    "# Loop through the folds\n",
    "for fold_num, (train_index, val_index) in enumerate(kf.split(X_train_augmented, y_train_encoded), 1):\n",
    "    X_train_fold, X_val_fold = X_train_augmented[train_index], X_train_augmented[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_encoded[train_index], y_train_encoded[val_index]\n",
    "\n",
    "    # Create and train the FNN model for each fold\n",
    "    fnn_model = create_fnn_model(input_shape_fnn)\n",
    "    history = fnn_model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        epochs=20, batch_size=32,\n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the training and validation data for each fold\n",
    "    _, train_accuracy = fnn_model.evaluate(X_train_fold, y_train_fold, verbose=0)\n",
    "    _, val_accuracy = fnn_model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "\n",
    "    print(f\"Fold {fold_num}: Training Accuracy = {train_accuracy:.4f}, Validation Accuracy = {val_accuracy:.4f}\")\n",
    "\n",
    "    # Store the training and validation accuracy for each fold\n",
    "    all_train_accuracy.append(train_accuracy)\n",
    "    all_val_accuracy.append(val_accuracy)\n",
    "\n",
    "# Calculate the average training and validation accuracy over all folds\n",
    "avg_train_accuracy = np.mean(all_train_accuracy)\n",
    "avg_val_accuracy = np.mean(all_val_accuracy)\n",
    "\n",
    "print(f\"\\nAverage Training Accuracy Across Folds = {avg_train_accuracy:.4f}\")\n",
    "print(f\"Average Validation Accuracy Across Folds = {avg_val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to calculate accuracy for a single keypoint\n",
    "def calculate_keypoint_accuracy(predicted_keypoint, ground_truth_keypoint, threshold):\n",
    "    distance = np.linalg.norm(np.array(predicted_keypoint) - np.array(ground_truth_keypoint))\n",
    "    return int(distance < threshold)\n",
    "\n",
    "# Define a function to calculate accuracy for all 33 landmarks\n",
    "def calculate_overall_accuracy(predicted_landmarks, ground_truth_landmarks, threshold):\n",
    "    num_landmarks = len(predicted_landmarks)\n",
    "    accuracies = [calculate_keypoint_accuracy(predicted_landmarks[i], ground_truth_landmarks[i], threshold) for i in range(num_landmarks)]\n",
    "    percentage_accurate = sum(accuracies) / num_landmarks * 100\n",
    "    return percentage_accurate\n",
    "\n",
    "# Usage\n",
    "threshold = 0.5  # Define your accuracy threshold\n",
    "predicted_landmarks = y_pred_classes_fnn  \n",
    "ground_truth_landmarks = y_test_encoded    \n",
    "\n",
    "accuracy = calculate_overall_accuracy(predicted_landmarks, ground_truth_landmarks, threshold)\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Testing: Paired T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Combine the training and validation accuracies for each fold into two arrays\n",
    "all_train_accuracy = [history['accuracy'][-1] for history in historical_performance]\n",
    "all_val_accuracy = [history['val_accuracy'][-1] for history in historical_performance]\n",
    "\n",
    "# Perform a paired sample t-test\n",
    "statistic, p_value = ttest_rel(all_train_accuracy, all_val_accuracy)\n",
    "\n",
    "# Print the results\n",
    "print(f't-statistic: {statistic}')\n",
    "print(f'P-value: {p_value}')\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a significant difference between training and validation accuracies.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference between training and validation accuracies.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-statistic: -11.208570000845578\n",
    "\n",
    "The t-statistic is a measure of how far the sample mean (in this case, the difference between training and validation accuracies) is from the population mean, in terms of standard errors. A larger absolute t-statistic suggests a greater difference between the sample means.\n",
    "\n",
    "In your case, the negative sign indicates that, on average, the validation accuracies are lower than the training accuracies.\n",
    "\n",
    "P-value: 0.00036078544601951236\n",
    "\n",
    "The p-value is the probability of observing a t-statistic as extreme as the one calculated, assuming that the null hypothesis is true. A lower p-value indicates stronger evidence against the null hypothesis.\n",
    "\n",
    "In your case, the very low p-value (0.00036) suggests that the observed difference in accuracies is unlikely to have occurred by random chance alone.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The phrase \"Reject the null hypothesis\" means that there is enough evidence to conclude that the training and validation accuracies are significantly different.\n",
    "\n",
    "The phrase \"There is a significant difference between training and validation accuracies\" reinforces the rejection of the null hypothesis, indicating that the observed difference is not likely due to random fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment, Inference and HPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained FNN model\n",
    "fnn_model.save('saved_model/fnn_model')\n",
    "\n",
    "# Save the label encoder for later use\n",
    "with open('saved_model/label_encoder.pkl', 'wb') as le_file:\n",
    "    pickle.dump(label_encoder, le_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = tf.keras.models.load_model('saved_model/fnn_model')\n",
    "\n",
    "# Load the label encoder\n",
    "with open('saved_model/label_encoder.pkl', 'rb') as le_file:\n",
    "    loaded_label_encoder = pickle.load(le_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "# Start capturing video from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 for the default camera, adjust if necessary\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1200)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 500)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with BlazePose\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    # Recolor image back to BGR for rendering\n",
    "    frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Detect Taijiquan Stances (class)\n",
    "    if results.pose_landmarks:\n",
    "        # Extract Pose landmarks\n",
    "        pose_landmarks = results.pose_landmarks.landmark\n",
    "        pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose_landmarks]).flatten())\n",
    "\n",
    "        # Make Detections\n",
    "        X = pd.DataFrame([pose_row])\n",
    "\n",
    "        # Convert X to numpy array\n",
    "        input_data = X.to_numpy().astype(np.float32)\n",
    "\n",
    "        # Make predictions using the Keras model\n",
    "        predictions = loaded_model.predict(input_data)\n",
    "        body_language_class = np.argmax(predictions)\n",
    "        body_language_prob = predictions[0]\n",
    "\n",
    "        print(body_language_class, body_language_prob)\n",
    "\n",
    "        # Display Probability\n",
    "        cv2.putText(frame, 'PROB', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, str(round(body_language_prob[body_language_class], 2)),\n",
    "                    (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Display detected class\n",
    "        cv2.putText(frame, f'CLASS: {loaded_label_encoder.classes_[body_language_class]}', (10, 90),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "\n",
    "    # Check for exit key (q)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "# Start capturing video from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 for the default camera, adjust if necessary\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1200)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 800)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with BlazePose\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    # Recolor image back to BGR for rendering\n",
    "    frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Detect Taijiquan Stances (class)\n",
    "    if results.pose_landmarks:\n",
    "        # Extract Pose landmarks\n",
    "        pose_landmarks = results.pose_landmarks.landmark\n",
    "        pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose_landmarks]).flatten())\n",
    "\n",
    "        # Make Detections\n",
    "        X = pd.DataFrame([pose_row])\n",
    "\n",
    "        # Convert X to numpy array\n",
    "        input_data = X.to_numpy().astype(np.float32)\n",
    "\n",
    "        # Run inference using the loaded FNN model\n",
    "        # Make predictions using the Keras model\n",
    "        predictions = loaded_model.predict(input_data)\n",
    "        body_language_class = np.argmax(predictions)\n",
    "        body_language_prob = predictions[0] if len(predictions.shape) == 2 else predictions\n",
    "\n",
    "        #print(body_language_class, body_language_prob)\n",
    "\n",
    "        # Convert landmark coordinates to integers\n",
    "        landmarks_as_pixels = np.array([(int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])) for landmark in results.pose_landmarks.landmark])\n",
    "\n",
    "        # Calculate bounding rectangle\n",
    "        bbox_c = cv2.boundingRect(landmarks_as_pixels)\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(frame, (int(bbox_c[0]), int(bbox_c[1])), (int(bbox_c[0] + bbox_c[2]), int(bbox_c[1] + bbox_c[3])), (0, 255, 0), 2)\n",
    "\n",
    "        # Display Probability\n",
    "        cv2.putText(frame, 'PROB', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, str(round(body_language_prob[0, body_language_class], 2)),\n",
    "                    (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display detected class\n",
    "        cv2.putText(frame, f'CLASS: {body_language_class}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(0, 191, 255), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "        # Add class label on the bounding box\n",
    "        cv2.putText(frame, f'CLASS: {body_language_class}', (int(bbox_c[0]), int(bbox_c[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "\n",
    "    # Check for exit key (q)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
