{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply landmark-based augmentation\n",
    "def apply_landmark_augmentation(landmarks, angle_range=(-10, 10), scale_range=(0.9, 1.1)):\n",
    "    augmented_landmarks = landmarks.copy()\n",
    "    num_landmarks = landmarks.shape[0]\n",
    "\n",
    "    # Apply augmentation to each landmark\n",
    "    for i in range(num_landmarks):\n",
    "        angle = np.random.uniform(angle_range[0], angle_range[1])\n",
    "        rotation_matrix = np.array([[np.cos(np.radians(angle)), -np.sin(np.radians(angle))],\n",
    "                                    [np.sin(np.radians(angle)), np.cos(np.radians(angle))]])\n",
    "        augmented_landmarks[i, :2] = np.dot(augmented_landmarks[i, :2], rotation_matrix.T)\n",
    "        scale_factor = np.random.uniform(scale_range[0], scale_range[1])\n",
    "        augmented_landmarks[i, :2] *= scale_factor\n",
    "\n",
    "    return augmented_landmarks.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('8taijiquan.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values in the 'class' column\n",
    "unique_classes = df['class'].unique()\n",
    "\n",
    "# Display the unique values\n",
    "print(\"Unique Classes:\", unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of classes\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['class']=='Horse Stance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df.drop('class', axis=1), df['class'], test_size=0.3, random_state=1234, stratify=df['class'])\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1234, stratify=y_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply landmark-based augmentation to X_train\n",
    "X_train_augmented = []\n",
    "for index, row in X_train.iterrows():\n",
    "    landmarks = np.array(row).reshape(-1, 4)  # Assuming each row represents landmarks for one sample\n",
    "    augmented_landmarks = apply_landmark_augmentation(landmarks)\n",
    "    X_train_augmented.append(augmented_landmarks)\n",
    "\n",
    "X_train_augmented = pd.DataFrame(X_train_augmented, columns=X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode class labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FNN model with modifications\n",
    "def create_fnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=input_shape))\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(layers.Dropout(0.7))\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(layers.Dropout(0.7))\n",
    "    model.add(layers.Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "    optimizer = Adam(learning_rate=0.0001)  # Adjust the learning rate as needed\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the FNN model with early stopping\n",
    "input_shape_fnn = (X_train_augmented.shape[1],)\n",
    "fnn_model = create_fnn_model(input_shape_fnn)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "fnn_history = fnn_model.fit(X_train_augmented, y_train_encoded, epochs=100, batch_size=32,\n",
    "                             validation_data=(X_val, y_val_encoded), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fnn_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(fnn_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the FNN model on the test set\n",
    "test_loss_fnn, test_accuracy_fnn = fnn_model.evaluate(X_test, y_test_encoded)\n",
    "print(f'Test Accuracy (FNN): {test_accuracy_fnn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for FNN\n",
    "y_pred_fnn = fnn_model.predict(X_test)\n",
    "y_pred_classes_fnn = np.argmax(y_pred_fnn, axis=1)\n",
    "conf_matrix_fnn = confusion_matrix(y_test_encoded, y_pred_classes_fnn)\n",
    "sns.heatmap(conf_matrix_fnn, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (FNN)')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report for FNN\n",
    "class_report_fnn = classification_report(y_test_encoded, y_pred_classes_fnn)\n",
    "print('Classification Report (FNN):')\n",
    "print(class_report_fnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "cv_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(df.drop('class', axis=1), df['class']):\n",
    "    X_train_cv, X_test_cv = df.drop('class', axis=1).iloc[train_index], df.drop('class', axis=1).iloc[test_index]\n",
    "    y_train_cv, y_test_cv = df['class'].iloc[train_index], df['class'].iloc[test_index]\n",
    "\n",
    "    model_cv = create_fnn_model((X_train_cv.shape[1],))\n",
    "    model_cv.fit(X_train_cv, label_encoder.transform(y_train_cv), epochs=50, batch_size=32,\n",
    "                 validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    _, accuracy_cv = model_cv.evaluate(X_test_cv, label_encoder.transform(y_test_cv))\n",
    "    cv_scores.append(accuracy_cv)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean Accuracy:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to TensorFlow Lite format (optional)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(fnn_model)\n",
    "tflite_model = converter.convert()\n",
    "with open('model_fnn.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the FNN model\n",
    "saved_model_path_fnn = './saved_model_fnn'\n",
    "loaded_fnn_model = tf.keras.models.load_model(saved_model_path_fnn)\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "tflite_model_path = './model_rnn.tflite'  # Adjust the path accordingly\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Initialize BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Start capturing video from the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1200)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 800)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with BlazePose\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    # Recolor image back to BGR for rendering\n",
    "    frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Detect Taijiquan Stances\n",
    "    if results.pose_landmarks:\n",
    "        # Extract Pose landmarks\n",
    "        pose_landmarks = results.pose_landmarks.landmark\n",
    "        pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose_landmarks]).flatten())\n",
    "\n",
    "        # Make Detections\n",
    "        X = pd.DataFrame([pose_row])\n",
    "\n",
    "        # Convert X to numpy array\n",
    "        input_data = X.to_numpy().astype(np.float32)\n",
    "\n",
    "        # Set the input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the output tensor\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        # Get the predicted class and probabilities\n",
    "        body_language_class = np.argmax(output_data)\n",
    "        body_language_prob = output_data[0]\n",
    "\n",
    "        print(body_language_class, body_language_prob)\n",
    "\n",
    "        # Display Probability\n",
    "        cv2.putText(frame, 'PROB', (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, str(round(body_language_prob[body_language_class], 2)),\n",
    "                    (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display detected class\n",
    "        cv2.putText(frame, f'CLASS: {body_language_class}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "        # Draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "    cv2.imshow('Pose Detection with ML', frame)\n",
    "\n",
    "    # Check for exit key\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
